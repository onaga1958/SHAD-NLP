{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vowpal Wabbit в NLP\n",
    "Автоматическая обработка текстов - 2017, семинар 4.\n",
    "\n",
    "В этом семинаре мы познакомимся с библиотекой Vowpal Wabbit и решим с его помощью задачу многоклассовой классификации на больших данных. \n",
    "Данные скачайте [здесь](https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/data) или запустите следующие две ячейки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы на семинаре не тратить время на обработку и обучение моделей на всех данных, предлагается использовать только небольшую подвыборку (`train-sample.csv`). Но сдавать ноутбук все равно необходимо с результатами на **всех** данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BodyMarkdown': \"I'm new to C#, and I want to use a trackbar for the forms opacity\\nThis is my code\\n\\n    decimal trans = trackBar1.Value / 5000\\n    this.Opacity = trans\\n\\nWhen I try to build it, I get this error\\n\\n**Cannot implicitly convert type 'decimal' to 'double**\\n\\nI tried making trans a double, but then the control doesn't work. This code worked fine for me in VB.NET. Any suggestions?\",\n",
       " 'OpenStatus': 'open',\n",
       " 'OwnerCreationDate': '07/31/2008 21:33:24',\n",
       " 'OwnerUndeletedAnswerCountAtPostTime': '0',\n",
       " 'OwnerUserId': '8',\n",
       " 'PostClosedDate': '',\n",
       " 'PostCreationDate': '07/31/2008 21:42:52',\n",
       " 'PostId': '4',\n",
       " 'ReputationAtPostCreation': '1',\n",
       " 'Tag1': 'c#',\n",
       " 'Tag2': '',\n",
       " 'Tag3': '',\n",
       " 'Tag4': '',\n",
       " 'Tag5': '',\n",
       " 'Title': 'Decimal vs Double?'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "INPUT_DATA = 'train.csv'\n",
    "# INPUT_DATA = 'train-sample.csv'\n",
    "\n",
    "reader = csv.DictReader(open(INPUT_DATA))\n",
    "dict(next(reader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый объект выборки соответствует некоторому посту на Stack Overflow. Требуется построить модель, определяющую статус поста. Подробнее про задачу и формат данных можно прочитать на [странице соревнования](https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow).\n",
    "\n",
    "Перед обучением модели из Vowpal Wabbit данные следует сохранить в специальный формат: <br>\n",
    "`label |namespace1 feature1:value1 feature2 feature3:value3 |namespace2 ...` <br>\n",
    "Записи `feature` и `feature:1.0` эквивалентны. Выделение признаков в смысловые подгруппы (namespaces) позволяет создавать взаимодействия между ними. Подробнее про формат входных данных можно прочитать [здесь](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format).\n",
    "\n",
    "Ниже реализована функция, которая извлекает признаки с помощью подаваемого на вход экстрактора, разбивает данные на трейн и тест и записывает их на диск."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATUSES = ['not a real question', 'not constructive', 'off topic', 'open', 'too localized']\n",
    "STATUS_DICT = {status: i+1 for i, status in enumerate(STATUSES)}\n",
    "\n",
    "def data2vw(features_extractor, input_file='train.csv',\n",
    "            train_output='train', test_output='test',\n",
    "            ytest_output='ytest'):\n",
    "    reader = csv.DictReader(open(input_file))\n",
    "    writer_train = open(train_output, 'w')\n",
    "    writer_test = open(test_output, 'w')\n",
    "    writer_ytest = open(ytest_output, 'w')\n",
    "    \n",
    "    for row in reader:\n",
    "        label = STATUS_DICT[row['OpenStatus']]\n",
    "        features = features_extractor(row)\n",
    "        output_line = '%s %s\\n' % (label, features)\n",
    "        if int(row['PostId']) % 2 == 0:\n",
    "            writer_train.write(output_line)\n",
    "        else:\n",
    "            writer_test.write(output_line)\n",
    "            writer_ytest.write('%s\\n' % label)\n",
    "            \n",
    "    writer_train.close()\n",
    "    writer_test.close()\n",
    "    writer_ytest.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с простейшей модели. В качестве признаков возьмите заголовки и очистите их: приведите символы к нижнему регистру, удалите пунктуацию. Также приветствуется использование стеммеров/лемматизаторов, однако учтите, что они могут сильно замедлить скорость обработки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 | decimal vs double\r\n",
      "4 | percentage width child in absolutely positioned parent doesnt work in ie\r\n",
      "4 | tools for porting j code to c\r\n",
      "4 | throw error in mysql trigger\r\n",
      "4 | whats the difference between mathfloor and mathtruncate\r\n"
     ]
    }
   ],
   "source": [
    "def extract_title(row):\n",
    "    title = row['Title'].lower()\n",
    "    title = re.sub('[^ a-z]', '', title)\n",
    "    return title\n",
    "\n",
    "\n",
    "data2vw(lambda row: '| %s' % extract_title(row))\n",
    "\n",
    "! head -n 5 train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим `vw` модель. Параметр `-d` отвечает за путь к обучающей выборке, `-f` – за путь к модели, `--oaa` – за режим мультиклассовой классификации `one-against-all`. Подробное описание всех параметров можно найти [здесь](https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments) или вызвав `vw --help`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = train\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        4        1        4\n",
      "0.500000 0.000000            2            2.0        4        4       12\n",
      "0.250000 0.000000            4            4.0        4        4        6\n",
      "0.125000 0.000000            8            8.0        4        4        7\n",
      "0.062500 0.000000           16           16.0        4        4        4\n",
      "0.062500 0.062500           32           32.0        4        4        9\n",
      "0.046875 0.031250           64           64.0        4        4        5\n",
      "0.062500 0.078125          128          128.0        4        4       11\n",
      "0.089844 0.117188          256          256.0        4        4        8\n",
      "0.070312 0.050781          512          512.0        4        4        5\n",
      "0.066406 0.062500         1024         1024.0        4        4        8\n",
      "0.063965 0.061523         2048         2048.0        4        4        6\n",
      "0.056641 0.049316         4096         4096.0        4        4       11\n",
      "0.053101 0.049561         8192         8192.0        4        4        6\n",
      "0.043518 0.033936        16384        16384.0        4        4       14\n",
      "0.033569 0.023621        32768        32768.0        4        4       15\n",
      "0.023666 0.013763        65536        65536.0        4        4        7\n",
      "0.016594 0.009521       131072       131072.0        4        4       12\n",
      "0.011841 0.007088       262144       262144.0        4        4       14\n",
      "0.011797 0.011753       524288       524288.0        4        4       11\n",
      "0.016258 0.020720      1048576      1048576.0        4        4       11\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1685253\n",
      "passes used = 1\n",
      "weighted example sum = 1685253.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.020987\n",
      "total feature number = 15261066\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим модель к тестовой выборке и сохраним предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "raw predictions = pred\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        4        4        9\n",
      "0.000000 0.000000            2            2.0        4        4        7\n",
      "0.000000 0.000000            4            4.0        4        4        8\n",
      "0.125000 0.250000            8            8.0        4        4        6\n",
      "0.125000 0.125000           16           16.0        4        4        7\n",
      "0.156250 0.187500           32           32.0        4        4       10\n",
      "0.093750 0.031250           64           64.0        4        4        8\n",
      "0.117188 0.140625          128          128.0        4        4        8\n",
      "0.125000 0.132812          256          256.0        4        4       15\n",
      "0.109375 0.093750          512          512.0        4        4       11\n",
      "0.095703 0.082031         1024         1024.0        4        4       10\n",
      "0.084473 0.073242         2048         2048.0        4        4       14\n",
      "0.065430 0.046387         4096         4096.0        4        4        7\n",
      "0.060791 0.056152         8192         8192.0        4        4        4\n",
      "0.050110 0.039429        16384        16384.0        4        4        6\n",
      "0.039825 0.029541        32768        32768.0        4        4       14\n",
      "0.029327 0.018829        65536        65536.0        4        4        8\n",
      "0.021118 0.012909       131072       131072.0        4        4       10\n",
      "0.015690 0.010262       262144       262144.0        4        4        6\n",
      "0.014833 0.013977       524288       524288.0        4        4        5\n",
      "0.018147 0.021460      1048576      1048576.0        4        4       11\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1685275\n",
      "passes used = 1\n",
      "weighted example sum = 1685275.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.022069\n",
      "total feature number = 15264878\n"
     ]
    }
   ],
   "source": [
    "! vw -i model -t test -r pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:-4.35192 2:-5.91851 3:-4.50567 4:2.55003 5:-6.68014\r\n",
      "1:-7.03341 2:-7.67406 3:-6.85027 4:4.34531 5:-6.29331\r\n",
      "1:-4.26167 2:-5.55281 3:-5.30194 4:2.89059 5:-7.50819\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 3 pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию, которая вычисляет `logloss` и `accuracy`, не загружая вектора в память. Используйте `softmax`, чтобы получить вероятности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(ytest_input='ytest', pred_input='pred'):\n",
    "    n, error, loss = 0, 0, 0\n",
    "    reader_ytest = open(ytest_input, 'r')\n",
    "    reader_pred = open(pred_input, 'r')\n",
    "\n",
    "    for label, pred in zip(reader_ytest, reader_pred):\n",
    "        n += 1\n",
    "        label = int(label) - 1\n",
    "        scores = [float(score.split(':')[1]) for score in pred.split(' ')]\n",
    "        scores = np.array(scores)\n",
    "        denominator = np.sum(np.exp(scores))\n",
    "        scores = np.exp(scores) / denominator\n",
    "        \n",
    "        prediction = np.argmax(scores) \n",
    "        \n",
    "        loss += np.log(scores[label])\n",
    "        if prediction != label:\n",
    "            error += 1\n",
    "        \n",
    "    reader_ytest.close()\n",
    "    reader_pred.close()\n",
    "    return - loss / n, 1 - float(error) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = \r\n",
      "num sources = 1\r\n",
      "\r\n",
      "\r\n",
      "VW options:\r\n",
      "  --random_seed arg                     seed random number generator\r\n",
      "  --ring_size arg                       size of example ring\r\n",
      "\r\n",
      "Update options:\r\n",
      "  -l [ --learning_rate ] arg            Set learning rate\r\n",
      "  --power_t arg                         t power value\r\n",
      "  --decay_learning_rate arg             Set Decay factor for learning_rate \r\n",
      "                                        between passes\r\n",
      "  --initial_t arg                       initial t value\r\n",
      "  --feature_mask arg                    Use existing regressor to determine \r\n",
      "                                        which parameters may be updated.  If no\r\n",
      "                                        initial_regressor given, also used for \r\n",
      "                                        initial weights.\r\n",
      "\r\n",
      "Weight options:\r\n",
      "  -i [ --initial_regressor ] arg        Initial regressor(s)\r\n",
      "  --initial_weight arg                  Set all weights to an initial value of \r\n",
      "                                        arg.\r\n",
      "  --random_weights arg                  make initial weights random\r\n",
      "  --input_feature_regularizer arg       Per feature regularization input file\r\n",
      "\r\n",
      "Parallelization options:\r\n",
      "  --span_server arg                     Location of server for setting up \r\n",
      "                                        spanning tree\r\n",
      "  --threads                             Enable multi-threading\r\n",
      "  --unique_id arg (=0)                  unique id used for cluster parallel \r\n",
      "                                        jobs\r\n",
      "  --total arg (=1)                      total number of nodes used in cluster \r\n",
      "                                        parallel job\r\n",
      "  --node arg (=0)                       node number in cluster parallel job\r\n",
      "\r\n",
      "Diagnostic options:\r\n",
      "  --version                             Version information\r\n",
      "  -a [ --audit ]                        print weights of features\r\n",
      "  -P [ --progress ] arg                 Progress update frequency. int: \r\n",
      "                                        additive, float: multiplicative\r\n",
      "  --quiet                               Don't output disgnostics and progress \r\n",
      "                                        updates\r\n",
      "  -h [ --help ]                         Look here: http://hunch.net/~vw/ and \r\n",
      "                                        click on Tutorial.\r\n",
      "\r\n",
      "Feature options:\r\n",
      "  --hash arg                            how to hash the features. Available \r\n",
      "                                        options: strings, all\r\n",
      "  --ignore arg                          ignore namespaces beginning with \r\n",
      "                                        character <arg>\r\n",
      "  --keep arg                            keep namespaces beginning with \r\n",
      "                                        character <arg>\r\n",
      "  --redefine arg                        redefine namespaces beginning with \r\n",
      "                                        characters of string S as namespace N. \r\n",
      "                                        <arg> shall be in form 'N:=S' where := \r\n",
      "                                        is operator. Empty N or S are treated \r\n",
      "                                        as default namespace. Use ':' as a \r\n",
      "                                        wildcard in S.\r\n",
      "  -b [ --bit_precision ] arg            number of bits in the feature table\r\n",
      "  --noconstant                          Don't add a constant feature\r\n",
      "  -C [ --constant ] arg                 Set initial value of constant\r\n",
      "  --ngram arg                           Generate N grams. To generate N grams \r\n",
      "                                        for a single namespace 'foo', arg \r\n",
      "                                        should be fN.\r\n",
      "  --skips arg                           Generate skips in N grams. This in \r\n",
      "                                        conjunction with the ngram tag can be \r\n",
      "                                        used to generate generalized \r\n",
      "                                        n-skip-k-gram. To generate n-skips for \r\n",
      "                                        a single namespace 'foo', arg should be\r\n",
      "                                        fN.\r\n",
      "  --feature_limit arg                   limit to N features. To apply to a \r\n",
      "                                        single namespace 'foo', arg should be \r\n",
      "                                        fN\r\n",
      "  --affix arg                           generate prefixes/suffixes of features;\r\n",
      "                                        argument '+2a,-3b,+1' means generate \r\n",
      "                                        2-char prefixes for namespace a, 3-char\r\n",
      "                                        suffixes for b and 1 char prefixes for \r\n",
      "                                        default namespace\r\n",
      "  --spelling arg                        compute spelling features for a give \r\n",
      "                                        namespace (use '_' for default \r\n",
      "                                        namespace)\r\n",
      "  --dictionary arg                      read a dictionary for additional \r\n",
      "                                        features (arg either 'x:file' or just \r\n",
      "                                        'file')\r\n",
      "  --dictionary_path arg                 look in this directory for \r\n",
      "                                        dictionaries; defaults to current \r\n",
      "                                        directory or env{PATH}\r\n",
      "  --interactions arg                    Create feature interactions of any \r\n",
      "                                        level between namespaces.\r\n",
      "  --permutations                        Use permutations instead of \r\n",
      "                                        combinations for feature interactions \r\n",
      "                                        of same namespace.\r\n",
      "  --leave_duplicate_interactions        Don't remove interactions with \r\n",
      "                                        duplicate combinations of namespaces. \r\n",
      "                                        For ex. this is a duplicate: '-q ab -q \r\n",
      "                                        ba' and a lot more in '-q ::'.\r\n",
      "  -q [ --quadratic ] arg                Create and use quadratic features\r\n",
      "  --q: arg                              : corresponds to a wildcard for all \r\n",
      "                                        printable characters\r\n",
      "  --cubic arg                           Create and use cubic features\r\n",
      "\r\n",
      "Example options:\r\n",
      "  -t [ --testonly ]                     Ignore label information and just test\r\n",
      "  --holdout_off                         no holdout data in multiple passes\r\n",
      "  --holdout_period arg                  holdout period for test only, default \r\n",
      "                                        10\r\n",
      "  --holdout_after arg                   holdout after n training examples, \r\n",
      "                                        default off (disables holdout_period)\r\n",
      "  --early_terminate arg                 Specify the number of passes tolerated \r\n",
      "                                        when holdout loss doesn't decrease \r\n",
      "                                        before early termination, default is 3\r\n",
      "  --passes arg                          Number of Training Passes\r\n",
      "  --initial_pass_length arg             initial number of examples per pass\r\n",
      "  --examples arg                        number of examples to parse\r\n",
      "  --min_prediction arg                  Smallest prediction to output\r\n",
      "  --max_prediction arg                  Largest prediction to output\r\n",
      "  --sort_features                       turn this on to disregard order in \r\n",
      "                                        which features have been defined. This \r\n",
      "                                        will lead to smaller cache sizes\r\n",
      "  --loss_function arg (=squared)        Specify the loss function to be used, \r\n",
      "                                        uses squared by default. Currently \r\n",
      "                                        available ones are squared, classic, \r\n",
      "                                        hinge, logistic and quantile.\r\n",
      "  --quantile_tau arg (=0.5)             Parameter \\tau associated with Quantile\r\n",
      "                                        loss. Defaults to 0.5\r\n",
      "  --l1 arg                              l_1 lambda\r\n",
      "  --l2 arg                              l_2 lambda\r\n",
      "  --named_labels arg                    use names for labels (multiclass, etc.)\r\n",
      "                                        rather than integers, argument \r\n",
      "                                        specified all possible labels, \r\n",
      "                                        comma-sep, eg \"--named_labels \r\n",
      "                                        Noun,Verb,Adj,Punc\"\r\n",
      "\r\n",
      "Output model:\r\n",
      "  -f [ --final_regressor ] arg          Final regressor\r\n",
      "  --readable_model arg                  Output human-readable final regressor \r\n",
      "                                        with numeric features\r\n",
      "  --invert_hash arg                     Output human-readable final regressor \r\n",
      "                                        with feature names.  Computationally \r\n",
      "                                        expensive.\r\n",
      "  --save_resume                         save extra state so learning can be \r\n",
      "                                        resumed later with new data\r\n",
      "  --save_per_pass                       Save the model after every pass over \r\n",
      "                                        data\r\n",
      "  --output_feature_regularizer_binary arg\r\n",
      "                                        Per feature regularization output file\r\n",
      "  --output_feature_regularizer_text arg Per feature regularization output file,\r\n",
      "                                        in text\r\n",
      "\r\n",
      "Output options:\r\n",
      "  -p [ --predictions ] arg              File to output predictions to\r\n",
      "  -r [ --raw_predictions ] arg          File to output unnormalized predictions\r\n",
      "                                        to\r\n",
      "\r\n",
      "Reduction options, use [option] --help for more info:\r\n",
      "\r\n",
      "  --bootstrap arg                       k-way bootstrap by online importance \r\n",
      "                                        resampling\r\n",
      "\r\n",
      "  --search arg                          Use learning to search, \r\n",
      "                                        argument=maximum action id or 0 for LDF\r\n",
      "\r\n",
      "  --replay_c arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --cbify arg                           Convert multiclass on <k> classes into \r\n",
      "                                        a contextual bandit problem\r\n",
      "\r\n",
      "  --cb_adf                              Do Contextual Bandit learning with \r\n",
      "                                        multiline action dependent features.\r\n",
      "\r\n",
      "  --cb arg                              Use contextual bandit learning with <k>\r\n",
      "                                        costs\r\n",
      "\r\n",
      "  --csoaa_ldf arg                       Use one-against-all multiclass learning\r\n",
      "                                        with label dependent features.  Specify\r\n",
      "                                        singleline or multiline.\r\n",
      "\r\n",
      "  --wap_ldf arg                         Use weighted all-pairs multiclass \r\n",
      "                                        learning with label dependent features.\r\n",
      "                                          Specify singleline or multiline.\r\n",
      "\r\n",
      "  --interact arg                        Put weights on feature products from \r\n",
      "                                        namespaces <n1> and <n2>\r\n",
      "\r\n",
      "  --csoaa arg                           One-against-all multiclass with <k> \r\n",
      "                                        costs\r\n",
      "\r\n",
      "  --multilabel_oaa arg                  One-against-all multilabel with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --log_multi arg                       Use online tree for multiclass\r\n",
      "\r\n",
      "  --ect arg                             Error correcting tournament with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --boosting arg                        Online boosting with <N> weak learners\r\n",
      "\r\n",
      "  --oaa arg                             One-against-all multiclass with <k> \r\n",
      "                                        labels\r\n",
      "\r\n",
      "  --top arg                             top k recommendation\r\n",
      "\r\n",
      "  --replay_m arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --binary                              report loss as binary classification on\r\n",
      "                                        -1,1\r\n",
      "\r\n",
      "  --link arg (=identity)                Specify the link function: identity, \r\n",
      "                                        logistic or glf1\r\n",
      "\r\n",
      "  --stage_poly                          use stagewise polynomial feature \r\n",
      "                                        learning\r\n",
      "\r\n",
      "  --lrqfa arg                           use low rank quadratic features with \r\n",
      "                                        field aware weights\r\n",
      "\r\n",
      "  --lrq arg                             use low rank quadratic features\r\n",
      "\r\n",
      "  --autolink arg                        create link function with polynomial d\r\n",
      "\r\n",
      "  --new_mf arg                          rank for reduction-based matrix \r\n",
      "                                        factorization\r\n",
      "\r\n",
      "  --nn arg                              Sigmoidal feedforward network with <k> \r\n",
      "                                        hidden units\r\n",
      "\r\n",
      "  --confidence                          Get confidence for binary predictions\r\n",
      "\r\n",
      "  --active_cover                        enable active learning with cover\r\n",
      "\r\n",
      "  --active                              enable active learning\r\n",
      "\r\n",
      "  --replay_b arg                        use experience replay at a specified \r\n",
      "                                        level [b=classification/regression, \r\n",
      "                                        m=multiclass, c=cost sensitive] with \r\n",
      "                                        specified buffer size\r\n",
      "\r\n",
      "  --bfgs                                use bfgs optimization\r\n",
      "\r\n",
      "  --conjugate_gradient                  use conjugate gradient based \r\n",
      "                                        optimization\r\n",
      "\r\n",
      "  --lda arg                             Run lda with <int> topics\r\n",
      "\r\n",
      "  --noop                                do no learning\r\n",
      "\r\n",
      "  --print                               print examples\r\n",
      "\r\n",
      "  --rank arg                            rank for matrix factorization.\r\n",
      "\r\n",
      "  --sendto arg                          send examples to <host>\r\n",
      "\r\n",
      "  --svrg                                Streaming Stochastic Variance Reduced \r\n",
      "                                        Gradient\r\n",
      "\r\n",
      "  --ftrl                                FTRL: Follow the Proximal Regularized \r\n",
      "                                        Leader\r\n",
      "\r\n",
      "  --pistol                              FTRL: Parameter-free Stochastic \r\n",
      "                                        Learning\r\n",
      "\r\n",
      "  --ksvm                                kernel svm\r\n",
      "\r\n",
      "Gradient Descent options:\r\n",
      "  --sgd                                 use regular stochastic gradient descent\r\n",
      "                                        update.\r\n",
      "  --adaptive                            use adaptive, individual learning \r\n",
      "                                        rates.\r\n",
      "  --invariant                           use safe/importance aware updates.\r\n",
      "  --normalized                          use per feature normalized updates\r\n",
      "  --sparse_l2 arg (=0)                  use per feature normalized updates\r\n",
      "\r\n",
      "Input options:\r\n",
      "  -d [ --data ] arg                     Example Set\r\n",
      "  --daemon                              persistent daemon mode on port 26542\r\n",
      "  --port arg                            port to listen on; use 0 to pick unused\r\n",
      "                                        port\r\n",
      "  --num_children arg                    number of children for persistent \r\n",
      "                                        daemon mode\r\n",
      "  --pid_file arg                        Write pid file in persistent daemon \r\n",
      "                                        mode\r\n",
      "  --port_file arg                       Write port used in persistent daemon \r\n",
      "                                        mode\r\n",
      "  -c [ --cache ]                        Use a cache.  The default is \r\n",
      "                                        <data>.cache\r\n",
      "  --cache_file arg                      The location(s) of cache_file.\r\n",
      "  -k [ --kill_cache ]                   do not reuse existing cache: create a \r\n",
      "                                        new one always\r\n",
      "  --compressed                          use gzip format whenever possible. If a\r\n",
      "                                        cache file is being created, this \r\n",
      "                                        option creates a compressed cache file.\r\n",
      "                                        A mixture of raw-text & compressed \r\n",
      "                                        inputs are supported with \r\n",
      "                                        autodetection.\r\n",
      "  --no_stdin                            do not default to reading from stdin\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! vw --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.14684\n",
      "accuracy = 0.97793\n"
     ]
    }
   ],
   "source": [
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На оригинальных данных `logloss` должен быть меньше `0.20`, `accuracy` больше `0.95`. Если это не так, то скорее всего у вас ошибка.\n",
    "\n",
    "Теперь попробуем улучшить модель, добавив новые признаки, порождаемые словами. В `vowpal wabbit` есть возможность делать это прямо на лету. Воспользуйтесь параметрами `affix`, `ngram`, `skips`.\n",
    "\n",
    "Далее везде при подборе параметров ориентируйтесь на улучшение `logloss`. Используйте `--quiet` или `-P`, чтобы избавиться от длинных выводов при обучении и применении моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.17138\n",
      "accuracy = 0.97581\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --ngram 2 --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.18119\n",
      "accuracy = 0.97577\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --ngram 2 --ngram 3 --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.17947\n",
      "accuracy = 0.97561\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --skips 1 --ngram 2 --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.16892\n",
      "accuracy = 0.97699\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --skips 2 --ngram 3 --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.16337\n",
      "accuracy = 0.97785\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --skips 3 --ngram 4 --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.16697\n",
      "accuracy = 0.97822\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --skips 3 --ngram 4 --ngram 5 --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.13856\n",
      "accuracy = 0.97847\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+1,-1' --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.13862\n",
      "accuracy = 0.97835\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2,-2' --quiet\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге только суффиксы дали улучшение, причем длины 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто качество `vw` модели получается учушить увеличением числа проходов по обучающей выборке (параметр `--passes`) и увеличением числа бит хэш-функции для уменьшения числа коллизий признаков (параметр `-b`). Подробнее про то, где в `vowpal wabbit` используется хэш-функция, можно прочитать [здесь](https://github.com/JohnLangford/vowpal_wabbit/wiki/Feature-Hashing-and-Extraction). Как меняется качество при изменении этих параметров? Верно ли, что при увеличении значений параметров `--passes` и `-b` качество всегда не убывает и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.13940\n",
      "accuracy = 0.97839\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2,-2' --quiet --passes 5 --cache_file cache_file\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.13771\n",
      "accuracy = 0.97815\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2,-2' --quiet --passes 3 --cache_file cache_file\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.16325\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2,-2' --quiet --passes 3 --cache_file cache_file -b 10\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.13663\n",
      "accuracy = 0.97807\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2,-2' --quiet --passes 3 --cache_file cache_file -b 20\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss  = 0.13642\n",
      "accuracy = 0.97801\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2,-2' --quiet --passes 3 --cache_file cache_file -b 25\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При увеличении passes результат вполне может и ухудшится, ведь при градиентном спуске нет гарантий, что лосс будет всегда убывать, мы знаем только, что он сойдется к какому-то локальному минимуму. При увеличении размера таблицы фичей мы можем столкнуться с недообучением, если сделаем её слишком большой (это, конечно, если у нас очень много признаков), так что тоже не не факт, что лосс всегда будет уменьшаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь интерес представляет то, какие признаки оказались наиболее важными для модели. Для этого сначала переведем модель в читаемый формат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 8.1.1\r\n",
      "Min label:-50.000000\r\n",
      "Max label:50.000000\r\n",
      "bits:25\r\n",
      "lda:0\r\n",
      "0 ngram: \r\n",
      "0 skip: \r\n",
      "options: --affix +2,-2 --oaa 5\r\n",
      "Checksum: 1485410803\r\n",
      ":0\r\n",
      "Constant:26094304:-2.752298\r\n",
      "Constant[1]:26094305:-3.298096\r\n",
      "Constant[2]:26094306:-3.059249\r\n",
      "Constant[3]:26094307:2.244808\r\n",
      "Constant[4]:26094308:-3.530444\r\n",
      "a:19615120:-0.021666\r\n",
      "a[1]:19615121:0.075131\r\n",
      "a[2]:19615122:-0.016544\r\n",
      "a[3]:19615123:0.007759\r\n",
      "a[4]:19615124:-0.008260\r\n",
      "aa:8686920:-1.165970\r\n",
      "aa[1]:8686921:-1.022695\r\n",
      "aa[2]:8686922:-0.967443\r\n",
      "aa[3]:8686923:1.049916\r\n",
      "aa[4]:8686924:-0.609711\r\n",
      "aaa:8584632:-0.167988\r\n",
      "aaa[1]:8584633:-0.472787\r\n",
      "aaa[2]:8584634:-0.640633\r\n",
      "aaa[3]:8584635:0.214563\r\n",
      "aaa[4]:8584636:-0.298246\r\n"
     ]
    }
   ],
   "source": [
    "! vw -i model -t --invert_hash model.readable train --quiet\n",
    "! head -n 30 model.readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые несколько строк соответствуют информации о модели. Далее следуют строчки вида `feature[label]:hash:weight`. Выделите для каждого класса 10 признаков с наибольшими по модулю весами. Постарайтесь сделать ваш алгоритм прохода по файлу константным по памяти. Например, можно воспользоваться [кучей](https://docs.python.org/2/library/heapq.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Определяем namedtuple Feature, содержащий имя, хэш и вес\n",
    "- Пишем функцию, которая из строчки достает Feature и label.\n",
    "- Пишем функцию, которая сравнивает два экземпляра Feature.\n",
    "- Пишем класс, который позволяет хранить топ объектов: он представляет из себя лист, в который элементы добавляются по убыванию, причем если их число превышает $n$ (передаваемое в конструкторе), то лист обрезается до $n$. Таким образом, память константа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature = namedtuple('Feature', ['name', 'hash', 'weight'])\n",
    "\n",
    "\n",
    "def get_feature_and_label(row):\n",
    "    row = row.strip().split(':')\n",
    "    if len(row) != 3:\n",
    "        return None, None\n",
    "    \n",
    "    ind = row[0].find('[')\n",
    "    if ind == -1:\n",
    "        label = 0\n",
    "        feature_name = row[0]\n",
    "    else:\n",
    "        label = int(row[0][ind + 1])\n",
    "        feature_name = row[0][:ind]\n",
    "\n",
    "    feature_hash = int(row[1])\n",
    "    feature_weight = float(row[2])\n",
    "    \n",
    "    return Feature(feature_name, feature_hash, feature_weight), label\n",
    "\n",
    "\n",
    "def compare(a, b):\n",
    "    return abs(a.weight) < abs(b.weight)\n",
    "\n",
    "\n",
    "class Top:\n",
    "    def __init__(self, n, compare):\n",
    "        \"\"\"\n",
    "        n - the max number element in top\n",
    "        compare(a, b) should represent a < b\n",
    "        \"\"\"\n",
    "        self._n = n\n",
    "        self._top = []\n",
    "        self._compare = compare\n",
    "        \n",
    "    def _insert(self, i, elem):\n",
    "        self._top.insert(i, elem)\n",
    "        if len(self._top) > self._n:\n",
    "                self._top.pop()\n",
    "\n",
    "    def add(self, elem):\n",
    "        for i in range(len(self._top) - 1, -1, -1):\n",
    "            if self._compare(elem, self._top[i]):\n",
    "                self._insert(i + 1, elem)\n",
    "                break\n",
    "        else:\n",
    "            self._insert(0, elem)\n",
    "\n",
    "    def get_top(self):\n",
    "        return self._top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_important = [Top(10, compare) for i in range(5)]\n",
    "with open('model.readable') as features_file:\n",
    "    for line in features_file:\n",
    "        feature, label = get_feature_and_label(line)\n",
    "        if feature is not None:\n",
    "            most_important[label].add(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not a real question\n",
      "Feature(name='mercurial', hash=32544120, weight=-4.177212)\n",
      "Feature(name='firing', hash=6468328, weight=-4.052338)\n",
      "Feature(name='uitableviewcell', hash=20184896, weight=-3.990192)\n",
      "Feature(name='activerecord', hash=5782720, weight=-3.91699)\n",
      "Feature(name='disabling', hash=10749160, weight=-3.89928)\n",
      "Feature(name='launching', hash=4314656, weight=-3.781238)\n",
      "Feature(name='subquery', hash=32226568, weight=-3.725806)\n",
      "Feature(name='groups', hash=30711456, weight=-3.6696)\n",
      "Feature(name='centering', hash=22058864, weight=-3.639237)\n",
      "Feature(name='forward', hash=12034776, weight=-3.481619)\n",
      "not constructive\n",
      "Feature(name='property', hash=5748833, weight=-3.546955)\n",
      "Feature(name='Constant', hash=26094305, weight=-3.298096)\n",
      "Feature(name='staticsized', hash=30737089, weight=-3.256423)\n",
      "Feature(name='match', hash=30737089, weight=-3.256423)\n",
      "Feature(name='webview', hash=22162041, weight=-3.187493)\n",
      "Feature(name='jqgrid', hash=8268369, weight=-3.113597)\n",
      "Feature(name='jar', hash=12536961, weight=-3.066759)\n",
      "Feature(name='section', hash=33160809, weight=-3.062937)\n",
      "Feature(name='visible', hash=18098129, weight=-2.993168)\n",
      "Feature(name='inserting', hash=31948409, weight=-2.971322)\n",
      "off topic\n",
      "Feature(name='listview', hash=21441882, weight=-4.113715)\n",
      "Feature(name='webview', hash=22162042, weight=-3.906676)\n",
      "Feature(name='target', hash=24741066, weight=-3.84304)\n",
      "Feature(name='symfony', hash=32661170, weight=-3.835181)\n",
      "Feature(name='edittext', hash=15411802, weight=-3.619089)\n",
      "Feature(name='nodes', hash=20503538, weight=-3.556359)\n",
      "Feature(name='tableview', hash=3969706, weight=-3.456325)\n",
      "Feature(name='gridview', hash=26141386, weight=-3.433071)\n",
      "Feature(name='constructor', hash=7190842, weight=-3.399217)\n",
      "Feature(name='linking', hash=2813066, weight=-3.354458)\n",
      "open\n",
      "Feature(name='classpath', hash=12133539, weight=3.099492)\n",
      "Feature(name='clipboard', hash=10443715, weight=3.090862)\n",
      "Feature(name='specifying', hash=25365875, weight=3.059187)\n",
      "Feature(name='branches', hash=11736547, weight=3.026906)\n",
      "Feature(name='centering', hash=22058867, weight=3.003681)\n",
      "Feature(name='movieclip', hash=12890571, weight=2.944242)\n",
      "Feature(name='viewmodel', hash=5001131, weight=2.904549)\n",
      "Feature(name='constructors', hash=32259443, weight=2.892482)\n",
      "Feature(name='singlethread', hash=5265827, weight=2.839021)\n",
      "Feature(name='radiobutton', hash=5265827, weight=2.839021)\n",
      "too localized\n",
      "Feature(name='render', hash=13296236, weight=-4.279674)\n",
      "Feature(name='connecting', hash=8466220, weight=-4.231206)\n",
      "Feature(name='member', hash=955980, weight=-4.149211)\n",
      "Feature(name='definition', hash=13867524, weight=-4.027293)\n",
      "Feature(name='clause', hash=16781684, weight=-3.978139)\n",
      "Feature(name='standard', hash=247740, weight=-3.97668)\n",
      "Feature(name='dynamicallynamed', hash=247740, weight=-3.97668)\n",
      "Feature(name='embed', hash=26747556, weight=-3.90324)\n",
      "Feature(name='dereferencing', hash=26747556, weight=-3.90324)\n",
      "Feature(name='cache', hash=10705284, weight=-3.90123)\n"
     ]
    }
   ],
   "source": [
    "for status, top_features in zip(STATUSES, most_important):\n",
    "    print(status)\n",
    "    for feature in top_features.get_top():\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Добавим признаки, извлеченные из текста поста (поле `BodyMarkdown`). В этом поле находится более подробная информация о вопросе, и часто туда помещают код, формулы и т.д. При удалении пунктуации мы потеряем много полезной информации, однако модель \"мешка слов\" на сырых данных может сильно раздуть признаковое пространство. В таких случаях работают с n-граммами на символах. <br>\n",
    "Будьте осторожны: символы \"`:`\" и \"`|`\" нельзя использовать в названиях признаков, поскольку они являются служебными для `vw`-формата. Замените эти символы на два других редко встречающихся в выборке (или вообще не встречающихся). Также не забудьте про \"`\\n`\". <br>\n",
    "Поскольку для каждого документа одна n-грамма может встретиться далеко не один раз, то будет экономнее записывать признаки в формате `[n-грамма]:[число вхождений]`.\n",
    "\n",
    "Также добавьте тэги (поля вид `TagN`). Приветствуется добавление информации о пользователе из других полей. Только не используйте `PostClosedDate` – в нем содержится информация о таргете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ngram_body(row, ngram=3):\n",
    "    body = row['BodyMarkdown'].strip().lower()\n",
    "    body = re.sub(':', 'ю', body)\n",
    "    body = re.sub('\\|', 'ф', body)\n",
    "    body = re.sub('\\n+', ' ', body)\n",
    "    \n",
    "    ngrams = [word[i:i+ngram]\n",
    "              for word in body.split(' ')\n",
    "              for i in range(len(word) - ngram + 1)]\n",
    "    counter = Counter(ngrams)\n",
    "    features = [ngram + ':' + str(cnt)\n",
    "                for ngram, cnt in counter.items()]\n",
    "    return ' '.join(features)\n",
    "\n",
    "def extract_tags(row):\n",
    "    i = 1\n",
    "    tags = []\n",
    "    while row.get('Tag' + str(i)):\n",
    "        tag = row['Tag' + str(i)]\n",
    "        if len(tag) > 0:\n",
    "            tags.append(tag)\n",
    "        i += 1\n",
    "    \n",
    "    return ' '.join(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим все вместе. Реализуйте экстрактор признаков, который выделяет каждую подгруппу в отдельный namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractors_list = [\n",
    "    ('t', extract_title), \n",
    "    ('b', extract_ngram_body), \n",
    "    ('a', extract_tags)\n",
    "] # (namespace, extractor)\n",
    "\n",
    "\n",
    "def make_feature_extractor(extractors_list):\n",
    "    def feature_extractor(row):\n",
    "        features = ['|' + namespace + ' ' + extractor(row)\n",
    "                    for namespace, extractor in extractors_list]\n",
    "        return ' '.join(features)\n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 |t decimal vs double |b i'm:1 new:1 c#,:1 and:1 wan:1 ant:1 use:1 tra:5 rac:2 ack:2 ckb:2 kba:2 bar:2 for:3 the:3 orm:1 rms:1 opa:2 pac:2 aci:2 cit:3 ity:2 thi:4 his:4 cod:2 ode:2 dec:2 eci:2 cim:2 ima:2 mal:2 ran:3 ans:3 ar1:1 r1.:1 1.v:1 .va:1 val:1 alu:1 lue:1 500:1 000:1 is.:1 s.o:1 .op:1 whe:1 hen:2 try:1 bui:1 uil:1 ild:1 it,:1 get:1 err:1 rro:1 ror:1 **c:1 *ca:1 can:1 ann:1 nno:1 not:1 imp:1 mpl:1 pli:1 lic:1 ici:1 itl:1 tly:1 con:2 onv:1 nve:1 ver:1 ert:1 typ:1 ype:1 'de:1 al':1 'do:1 dou:2 oub:2 ubl:2 ble:2 le*:1 e**:1 tri:1 rie:1 ied:1 mak:1 aki:1 kin:1 ing:1 le,:1 but:1 ont:1 ntr:1 tro:1 rol:1 doe:1 oes:1 esn:1 sn':1 n't:1 wor:2 ork:2 rk.:1 rke:1 ked:1 fin:1 ine:1 vb.:1 b.n:1 .ne:1 net:1 et.:1 any:1 sug:1 ugg:1 gge:1 ges:1 est:1 sti:1 tio:1 ion:1 ons:1 ns?:1 |a c#\r\n"
     ]
    }
   ],
   "source": [
    "data2vw(make_feature_extractor(extractors_list))\n",
    "\n",
    "! head -n 1 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.32839\n",
      "accuracy = 0.97926\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 --cache_file cache_file -b 25\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUG BEGIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 |t for mongodb is it better to reference an object or use a natural string key |b bui:1 uil:1 ild:1 ldi:1 din:1 ing:3 cor:1 orp:1 rpu:1 pus:1 ind:1 nde:1 dex:1 exe:1 xed:1 sen:2 ent:3 nte:2 ten:2 enc:4 nce:3 ces:2 dif:1 iff:1 ffe:1 fer:3 ere:4 ren:3 lan:5 ang:5 ngu:5 gua:5 uag:5 age:5 ges:2 es.:1 hav:2 ave:2 col:3 oll:3 lle:3 lec:3 ect:5 cti:5 tio:3 ion:3 whi:1 hic:1 ich:1 bot:1 oth:1 obj:2 bje:2 jec:2 tid:1 and:1 the:6 iso:1 cod:1 ode:1 key:2 ey.:1 bet:2 ett:1 tte:1 ter:1 use:1 ref:2 efe:2 sto:1 tor:1 ore:1 lik:1 ike:1 \"en:1 en\":1 \"fr:1 fr\":1 r\"?:1 \"?i:1 sup:1 upp:1 ppo:1 pos:1 ose:1 it':1 t's:1 com:1 omp:1 mpr:1 pro:1 rom:1 omi:1 mis:1 ise:1 etw:1 twe:1 wee:1 een:1 enю:1 eas:1 ase:1 nci:1 cin:1 tha:2 hat:2 spe:1 pee:1 eed:1 doi:1 oin:1 que:1 uer:1 eri:1 rie:1 ies:1 whe:1 her:1 has:1 cer:1 ert:1 rta:1 tai:1 ain:1 siz:1 ize:1 dat:1 ata:1 dis:1 isk:1 ska:1 kan:1 any:1 bes:1 est:1 pra:1 rac:1 act:1 tic:1 ice:1 sho:1 hou:1 oul:1 uld:1 kno:1 now:1 of?:1 |a mongodb\r\n"
     ]
    }
   ],
   "source": [
    "data2vw(make_feature_extractor(extractors_list), input_file='train-sample.csv',\n",
    "        train_output='train_small', test_output='test_small',\n",
    "        ytest_output='ytest_small')\n",
    "\n",
    "! head -n 1 train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = model_small\n",
      "Num weight bits = 25\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "vw: unrecognised option '--quite'\n",
      "only testing\n",
      "raw predictions = pred\n",
      "Num weight bits = 25\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = test_small\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        4        4      352\n",
      "0.000000 0.000000            2            2.0        4        4       93\n",
      "0.250000 0.500000            4            4.0        4        4      221\n",
      "0.375000 0.500000            8            8.0        3        1      102\n",
      "0.312500 0.250000           16           16.0        4        4       52\n",
      "0.375000 0.437500           32           32.0        4        4      258\n",
      "0.390625 0.406250           64           64.0        3        3      122\n",
      "0.335938 0.281250          128          128.0        1        4       92\n",
      "0.355469 0.375000          256          256.0        4        4      425\n",
      "0.355469 0.355469          512          512.0        3        3      162\n",
      "0.361328 0.367188         1024         1024.0        4        4      217\n",
      "0.356934 0.352539         2048         2048.0        4        1       89\n",
      "0.368896 0.380859         4096         4096.0        4        4      325\n",
      "0.366455 0.364014         8192         8192.0        4        4      395\n",
      "0.364319 0.362183        16384        16384.0        1        1       97\n",
      "0.362488 0.360657        32768        32768.0        4        4      114\n",
      "malformed example!\n",
      "words.size() = 1039\n",
      "0.364975 0.367462        65536        65536.0        4        4      735\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 69874\n",
      "passes used = 1\n",
      "weighted example sum = 69874.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.365601\n",
      "total feature number = 16832389\n",
      "logloss  = 1.05444\n",
      "accuracy = 0.61558\n"
     ]
    }
   ],
   "source": [
    "! vw -d train_small --loss_function logistic --oaa 5 -f model_small --affix '+2t,-2t' -b 25 --quite\n",
    "! vw -i model_small -t test_small -r pred\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEBUG END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэкспериментируйте с другими параметрами модели. Добавьте квадратичные взаимодействия между различными блоками признаков, измените параметры оптимизатора, добавьте регуляризацию и т.д. Совсем необязательно перебирать все параметры по сетке и добиваться оптимального качества. Для нас важнее то, насколько хорошо вы разобрались с возможностями библиотеки и продемонстрировали это.\n",
    "\n",
    "Выберите не менее трех параметров. Для каждого из них объясните, почему по вашему мнению его изменение может улучшить качество модели, подберите оптимальное значение. Можете перебрать несколько значений \"руками\", а можете воспользоваться [vw-hypersearch](https://github.com/JohnLangford/vowpal_wabbit/wiki/Using-vw-hypersearch) или [vw-hyperopt](https://github.com/JohnLangford/vowpal_wabbit/blob/master/utl/vw-hyperopt.py) ([статья на хабре](https://habrahabr.ru/company/dca/blog/272697/)). Какие параметры повлияли на улучшение качества сильнее всего?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-l, --decay_learning_rate (0,5, 0,75, 1), -q, --l1, --l2, --power_t (0, 1/2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбор расписания learning rate часто может существенно повлиять на успешность обучения, из-за нетривиальных функций, которые мы пытаемся оптимизировать. Поэтому попробуем поварьировать -l, --decay_learning_rate и --power_t. Так же в виду большого количества признаков многие из них неинформативы, поэтому необходимо использовать --l1, чтобы не переобучиться и чтобы отсеять несущественные. Ну и заодно можно попробовать поварьировать и --l2, ведь вычисления с ней по крайней мере быстрые)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же попробуем -q. -q bb - слишком долго, попробуем -q tb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.18073\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 -q tb\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.13734\n",
      "accuracy = 0.97926\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 -l 0.05\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.42243\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 -l 0.005\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.16870\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 -l 0.0005\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "самым эффективным окзался дефолтный -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.60944\n",
      "accuracy = 0.00904\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.1\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.60944\n",
      "accuracy = 0.00904\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.01\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.60944\n",
      "accuracy = 0.00904\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.60944\n",
      "accuracy = 0.00904\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.0001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.12815\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.15469\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.000001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.60914\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l2 0.1\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.60656\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l2 0.01\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.57784\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l2 0.001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.31926\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l2 0.0001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.70489\n",
      "accuracy = 0.97901\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l2 0.00001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.57784\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l2 0.0001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.70489\n",
      "accuracy = 0.97901\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l2 0.00001\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 оказалась бесполезной, а вот l1 очень неплохо уменьшила лосс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь подберем расписание learning rate'a. С дефолтными $decay = 1$ и $power\\_t = 0$ $loss = 0.128$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.12815\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --power_t 0.5\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.58016\n",
      "accuracy = 0.01690\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --power_t 1\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.12795\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --decay_learning_rate 0.75\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.12795\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --decay_learning_rate 0.75 --power_t 0.5\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.57922\n",
      "accuracy = 0.01690\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --decay_learning_rate 0.75 --power_t 1\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.12757\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --decay_learning_rate 0.5\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 0.12757\n",
      "accuracy = 0.97927\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --decay_learning_rate 0.5 --power_t 0.5\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malformed example!\n",
      "words.size() = 7342\n",
      "logloss  = 1.33554\n",
      "accuracy = 0.03956\n"
     ]
    }
   ],
   "source": [
    "! vw -d train --loss_function logistic --oaa 5 -f model --affix '+2t,-2t' --quiet --passes 3 \\\n",
    "--cache_file cache_file -b 25 --l1 0.00001 --decay_learning_rate 0.5 --power_t 1\n",
    "! vw -i model -t test -r pred --quiet\n",
    "print('logloss  = %.5f\\naccuracy = %.5f' % get_scores())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--l1 0.00001 очень сильно улучшил модель. До регуляризации результаты были хуже, чем у модели, опиравшейся только на title. После добавления регуляризации лосс уменьшился в 3 раза и стал меньше, чем был у простой модели. Расписание learning rate помогло не так сильно, но тоже сумело немного уменьшить лосс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге лучшие параметры: --affix '+2t,-2t' --passes 3 -b 25 --l1 0.00001 --decay_learning_rate 0.5 --power_t 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
